{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización del modelo de lenguaje y del buffer de conversación\n",
    "llm = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "conversation_buffer = ConversationBufferMemory()\n",
    "\n",
    "# Creación de la ConversationChain con el buffer de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=conversation_buffer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: hello there\\nAI: Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseChatMemory.clear of ConversationBufferMemory()>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.memory.clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ignac\\Documents\\chat-with-rag-using-langchain-nextjs-and-fastapi\\backend\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hello there',\n",
       " 'history': '',\n",
       " 'response': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Im fine, thank yo',\n",
       " 'history': \"Human: hello there\\nAI: Hello! How can I assist you today?\\nHuman: Im fine, thank yo\\nAI: You're welcome! Is there anything specific you would like to chat about or any questions I can help answer?\\nHuman: hello there\\nAI: Hello! How can I assist you today? Is there anything specific you would like to chat about or any questions I can help answer?\",\n",
       " 'response': \"You're welcome! I'm glad to hear that you're doing well. If you have any questions or if there's anything specific you would like to chat about, feel free to let me know.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain(\"Im fine, thank yo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.encode(conversation_buffer.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_messages(conversation_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[('chat_memory', ChatMessageHistory(messages=[HumanMessage(content='hello there'), AIMessage(content='Hello! How can I assist you today?'), HumanMessage(content='Im fine, thank yo'), AIMessage(content=\"You're welcome! Is there anything specific you would like to chat about or any questions I can help answer?\"), HumanMessage(content='hello there'), AIMessage(content='Hello! How can I assist you today? Is there anything specific you would like to chat about or any questions I can help answer?'), HumanMessage(content='Im fine, thank yo'), AIMessage(content=\"You're welcome! I'm glad to hear that you're doing well. If you have any questions or if there's anything specific you would like to chat about, feel free to let me know.\")])), ('output_key', None), ('input_key', None), ('return_messages', False), ('human_prefix', 'Human'), ('ai_prefix', 'AI'), ('memory_key', 'history')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para añadir un nuevo input y generar una respuesta\n",
    "def ask_question(input_text):\n",
    "    # Añadir el input al historial\n",
    "    conversation_chain.conversation_buffer.add_user_input(input_text)\n",
    "    \n",
    "    # Generar la respuesta utilizando el historial actual\n",
    "    response = conversation_chain.generate_response(input_text)\n",
    "    \n",
    "    # Añadir la respuesta al historial\n",
    "    conversation_chain.conversation_buffer.add_model_response(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Ejemplo de interacción\n",
    "history = [\n",
    "    {\"input\": \"¿Cuál es la capital de Francia?\", \"docs\": None},\n",
    "    {\"input\": \"¿Y qué famoso monumento se encuentra allí?\", \"docs\": None}\n",
    "]\n",
    "\n",
    "# Pasar el historial al buffer de conversación\n",
    "for item in history:\n",
    "    conversation_chain.conversation_buffer.add_user_input(item[\"input\"])\n",
    "    # Aquí se omiten las respuestas del modelo para simplificar, pero normalmente se añadirían también.\n",
    "\n",
    "# Hacer una nueva pregunta que dependa del contexto anterior\n",
    "new_question = \"¿Cuánto mide ese monumento?\"\n",
    "response = ask_question(new_question)\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
